{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655aba71-7521-41d7-8a25-d1587052988a",
   "metadata": {},
   "source": [
    "# Traffic accident severity prediction task\n",
    "\n",
    "Author: Baixiang Huang <bhuang15@hawk.iit.edu>, Bryan Hooi <bhooi@comp.nus.edu.sg>, Kai Shu <kshu@iit.edu>\n",
    "\n",
    "License: GPL3\n",
    "\n",
    "TAP: A Comprehensive Data Repository for Traffic Accident Prediction in Transportation Networks. Baixiang Huang, Bryan Hooi, Kai Shu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae266b3-599f-4e4c-bcbb-85aa9ddf7219",
   "metadata": {
    "id": "dae266b3-599f-4e4c-bcbb-85aa9ddf7219",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import networkx as nx\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.io import read_npz\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import reset, uniform, zeros\n",
    "from torch_geometric.typing import OptTensor, OptPairTensor, Adj, Size\n",
    "from torch_geometric.data import Data, DataLoader, InMemoryDataset, download_url\n",
    "\n",
    "from pylab import cm\n",
    "from matplotlib import colors\n",
    "from IPython.display import clear_output\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from typing import Union, Tuple, Callable, Optional\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, roc_auc_score\n",
    "\n",
    "np.random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "plt.style.use(\"ggplot\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "niEvdKjacdWr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "niEvdKjacdWr",
    "outputId": "64eb4a18-cb0c-42c3-b2d1-4df417293edc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 11.6\n",
      "PyTorch version: 1.12.1+cu116\n",
      "PyG version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "print('CUDA version:', torch.version.cuda)\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('PyG version:', torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d316552-5234-4c6c-8e8a-2b9f278be729",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), 0, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "device, torch.cuda.current_device(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66833443-e0b8-4b04-8541-21ff794b89c3",
   "metadata": {
    "id": "66833443-e0b8-4b04-8541-21ff794b89c3"
   },
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c476a58-9987-4928-af4b-dc630bf16a43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\", \"Alaska\": \"AK\", \"Arizona\": \"AZ\", \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\", \"Colorado\": \"CO\", \"Connecticut\": \"CT\", \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\", \"Georgia\": \"GA\", \"Hawaii\": \"HI\", \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\", \"Indiana\": \"IN\", \"Iowa\": \"IA\", \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\", \"Louisiana\": \"LA\", \"Maine\": \"ME\", \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\", \"Michigan\": \"MI\", \"Minnesota\": \"MN\", \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\", \"Montana\": \"MT\", \"Nebraska\": \"NE\", \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\", \"New Jersey\": \"NJ\", \"New Mexico\": \"NM\", \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\", \"North Dakota\": \"ND\", \"Ohio\": \"OH\", \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\", \"Pennsylvania\": \"PA\", \"Rhode Island\": \"RI\", \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\", \"Tennessee\": \"TN\", \"Texas\": \"TX\", \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\", \"Virginia\": \"VA\", \"Washington\": \"WA\", \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\", \"Wyoming\": \"WY\", \"District of Columbia\": \"DC\", \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\", \"Northern Mariana Islands\": \"MP\", \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\", \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "    \n",
    "us_abbrev_to_state = dict(map(reversed, us_state_to_abbrev.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84246c34-414d-4d74-85a5-cfbeb3b11446",
   "metadata": {
    "id": "84246c34-414d-4d74-85a5-cfbeb3b11446",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_npz(path):\n",
    "    with np.load(path, allow_pickle=True) as f:\n",
    "        return parse_npz(f)\n",
    "\n",
    "    \n",
    "def parse_npz(f):\n",
    "    \"Set up severity prediction task here: use severity_labels as labels\"\n",
    "    x = torch.from_numpy(f['x']).to(torch.float)\n",
    "    occur_labels = torch.from_numpy(f['occur_labels']).to(torch.long)\n",
    "    edge_attr = torch.from_numpy(f['edge_attr']).to(torch.float)\n",
    "    edge_index = torch.from_numpy(f['edge_index']).to(torch.long).t().contiguous()\n",
    "    edge_attr_dir = torch.from_numpy(f['edge_attr_dir']).to(torch.float)\n",
    "    edge_attr_ang = torch.from_numpy(f['edge_attr_ang']).to(torch.float)\n",
    "    coords = torch.from_numpy(f['coordinates']).to(torch.float)\n",
    "    cnt_labels = torch.from_numpy(f['cnt_labels']).to(torch.long)\n",
    "    severity_labels = torch.from_numpy(f['severity_8labels']).to(torch.long)\n",
    "    return Data(x=x, y=severity_labels, occur_labels=occur_labels, edge_index=edge_index, edge_attr=edge_attr, \n",
    "                edge_attr_dir=edge_attr_dir, edge_attr_ang=edge_attr_ang, coords=coords, cnt_labels=cnt_labels)\n",
    "\n",
    "\n",
    "def train_test_split_stratify(dataset, train_ratio, val_ratio, class_num):\n",
    "    labels = dataset[0].y\n",
    "    train_mask = torch.zeros(size=labels.shape, dtype=bool)\n",
    "    val_mask = torch.zeros(size=labels.shape, dtype=bool)\n",
    "    test_mask = torch.zeros(size=labels.shape, dtype=bool)\n",
    "    for i in range(class_num):\n",
    "        stratify_idx = np.argwhere(labels.numpy() == i).flatten()\n",
    "        np.random.shuffle(stratify_idx)\n",
    "        split1 = int(len(stratify_idx) * train_ratio)\n",
    "        split2 = split1 + int(len(stratify_idx) * val_ratio)\n",
    "        train_mask[stratify_idx[:split1]] = True \n",
    "        val_mask[stratify_idx[split1:split2]] = True\n",
    "        test_mask[stratify_idx[split2:]] = True\n",
    "    \n",
    "    highest = pd.DataFrame(labels).value_counts().head().iloc[0]\n",
    "    # print(\"Null Accuracy:\", highest / (len(labels)))\n",
    "    return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fdf4197-750c-4515-8a03-849fa4164a0c",
   "metadata": {
    "id": "0fdf4197-750c-4515-8a03-849fa4164a0c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TRAVELDataset(InMemoryDataset):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        name (string): The name of the dataset.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    url = 'https://github.com/baixianghuang/travel/raw/main/TAP-city/{}.npz'\n",
    "    \n",
    "    def __init__(self, root: str, name: str,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        self.name = name.lower()\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> str:\n",
    "        return f'{self.name}.npz'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        download_url(self.url.format(self.name), self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        data = read_npz(self.raw_paths[0])\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.name.capitalize()}Full()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbfd29ad-0c2c-4c1c-9d50-5e1e4190b9eb",
   "metadata": {
    "id": "cbfd29ad-0c2c-4c1c-9d50-5e1e4190b9eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TRAVELConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample, or :obj:`-1` to\n",
    "            derive the size from the first input(s) to the forward method.\n",
    "            A tuple corresponds to the sizes of source and target\n",
    "            dimensionalities.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        nn (torch.nn.Module): A neural network that\n",
    "            maps feature data of shape :obj:`[-1,\n",
    "            num_node_features + num_edge_features]` to shape\n",
    "            :obj:`[-1, new_dimension]`, *e.g.*, defined by\n",
    "            :class:`torch.nn.Sequential`.\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
    "            (default: :obj:`\"add\"`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add the transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
    "                 out_channels: int, nn: Callable, aggr: str = 'add',\n",
    "                 root_weight: bool = True, bias: bool = True, **kwargs):\n",
    "        super(TRAVELConv, self).__init__(aggr=aggr, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.nn = nn\n",
    "        self.aggr = aggr\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "            \n",
    "        self.in_channels_l = in_channels[0]\n",
    "\n",
    "        if root_weight:\n",
    "            self.root = Parameter(torch.Tensor(in_channels[1], out_channels))\n",
    "        else:\n",
    "            self.register_parameter('root', None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "        if self.root is not None:\n",
    "            uniform(self.root.size(0), self.root)\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if x_r is not None and self.root is not None:\n",
    "            out += torch.matmul(x_r, self.root)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_i: Tensor, x_j: Tensor, edge_attr: Tensor) -> Tensor:     \n",
    "        inputs = torch.cat([x_j, edge_attr], dim=1)\n",
    "        return self.nn(inputs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, aggr=\"{}\", nn={})'.format(self.__class__.__name__,\n",
    "                                                     self.in_channels,\n",
    "                                                     self.out_channels,\n",
    "                                                     self.aggr, self.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d99aab-7311-41a1-842c-336b14064d43",
   "metadata": {
    "id": "OIas8pewQiG8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    logits, measures = model().detach(), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        mea = f1_score(data.y[mask].cpu(), pred.cpu(), average='weighted')\n",
    "        measures.append(mea)\n",
    "    label_pred = logits.max(1)[1]\n",
    "\n",
    "    mask = data.test_mask\n",
    "    scores = logits[mask][:, 1]\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    test_y = data.y[mask]\n",
    "    \n",
    "    test_acc = pred.eq(test_y).sum().item() / mask.sum().item()\n",
    "    return measures, label_pred, test_acc\n",
    "\n",
    "\n",
    "def train_loop(model, data, optimizer, num_epochs, model_name='', city_name=''):\n",
    "    epochs, train_measures, valid_measures, test_measures, test_accs = [], [], [], [], []\n",
    "    coords = data.coords.cpu().numpy()\n",
    "    gdf_pred = pd.DataFrame({'x': coords[:, 0], 'y': coords[:, 1]})\n",
    "    for epoch in range(num_epochs):\n",
    "        train(model, data, optimizer)\n",
    "        log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "        measures, label_pred, test_acc = test(model, data)\n",
    "        train_mea, valid_mea, test_mea = measures\n",
    "        epochs.append(epoch)\n",
    "        train_measures.append(train_mea)\n",
    "        valid_measures.append(valid_mea)\n",
    "        test_measures.append(test_mea)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            clear_output(True)\n",
    "            fig, (ax1, ax) = plt.subplots(1, 2, figsize=(30, 12))  \n",
    "            gdf_pred['label'] = label_pred.cpu().numpy()\n",
    "            for i in range(class_num):\n",
    "                G = nx.MultiGraph()\n",
    "                G.add_nodes_from(gdf_pred[gdf_pred['label'] == i].index)\n",
    "                sub1 = nx.draw(G, pos=pos_dict, ax=ax1, node_color=colors.rgb2hex(cmap(i)), node_size=10, label=i)\n",
    "\n",
    "            ax.text(1, 1, log.format(epoch, train_measures[-1], valid_measures[-1], test_measures[-1]), fontsize=18)\n",
    "            ax.plot(epochs, train_measures, \"r\", epochs, valid_measures, \"g\", epochs, test_measures, \"b\")\n",
    "            ax.set_ylim([0, 1])\n",
    "            ax.legend([\"train\", \"valid\", \"test\"])  \n",
    "            norm = colors.BoundaryNorm(boundaries=range(8), ncolors=8)\n",
    "            plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax1, orientation='vertical', label='Severity') \n",
    "            ax1.set_title(city_name+' '+model_name, y=-0.01)\n",
    "            plt.show()\n",
    "\n",
    "    select_idx = np.argmax(valid_measures[num_epochs//2:]) + num_epochs//2\n",
    "    final_test_mea = np.array(test_measures)[select_idx]\n",
    "    final_test_acc = np.array(test_accs)[select_idx]\n",
    "    print('F measure {:.5f} | Test Accuracy {:.5f}'.format(final_test_mea, final_test_acc))\n",
    "    return (round(final_test_mea*100, 2), round(final_test_acc*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34c8d2-4fe6-4b4a-856c-c247ca68c822",
   "metadata": {
    "id": "bc34c8d2-4fe6-4b4a-856c-c247ca68c822",
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1fd107-e02f-4c9e-9f02-4da1c5e8951b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cities_sorted_by_accident.pkl is available in the directory `util`\n",
    "# with open('cities_sorted_by_accident.pkl', 'rb') as fp:\n",
    "#     all_city_ls = pickle.load(fp)\n",
    "    \n",
    "# print('# cities:', len(all_city_ls))\n",
    "# for e in all_city_ls[:50]:\n",
    "#     print(e[0]+' ('+e[1]+')', end = ', ')\n",
    "# len(all_city_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc75e7-830a-4774-8698-7ce6d44e9d68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_with_labels(df_nodes, model_name='test'):\n",
    "    plt.figure(figsize=(6, 5)) \n",
    "    for i in range(class_num):\n",
    "        G = nx.MultiGraph()\n",
    "        G.add_nodes_from(df_nodes[df_nodes['label'] == i].index)\n",
    "        nx.draw(G, pos=pos_dict, node_color=colors.rgb2hex(cmap(i)), node_size=3, label=i)\n",
    "    norm = colors.BoundaryNorm(boundaries=range(8), ncolors=8)\n",
    "    plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), orientation='vertical', label='Severity')\n",
    "    plt.title(model_name, y=-0.01)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "d=16\n",
    "p=0.5\n",
    "all_res = []\n",
    "class_num = 8\n",
    "num_epochs = 301\n",
    "cmap = cm.get_cmap('cool', class_num)\n",
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a1f46-fd55-47b6-960d-3373307951d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim=d):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(dataset.num_features, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x = F.relu(self.fc1(data.x))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=d):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(dataset.num_features, hidden_dim)\n",
    "        self.conv2 = pyg_nn.GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class ChebNet(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=d):\n",
    "        super(ChebNet, self).__init__()\n",
    "        self.conv1 = pyg_nn.ChebConv(dataset.num_features, hidden_dim, K=2)\n",
    "        self.conv2 = pyg_nn.ChebConv(hidden_dim, hidden_dim, K=2)\n",
    "        self.fc1 = nn.Linear(hidden_dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class ARMANet(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=d):\n",
    "        super(ARMANet, self).__init__()\n",
    "        self.conv1 = pyg_nn.ARMAConv(dataset.num_features, hidden_dim)\n",
    "        self.conv2 = pyg_nn.ARMAConv(hidden_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, dim=d):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = pyg_nn.SAGEConv(dataset.num_features, dim)\n",
    "        self.conv2 = pyg_nn.SAGEConv(dim, dim*2, normalize=True)\n",
    "        self.fc1 = nn.Linear(dim*2, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class TAGCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=d):\n",
    "        super(TAGCN, self).__init__()\n",
    "        self.conv1 = pyg_nn.TAGConv(dataset.num_features, hidden_dim)\n",
    "        self.conv2 = pyg_nn.TAGConv(hidden_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, dim=d):\n",
    "        super(GIN, self).__init__()\n",
    "        nn1 = nn.Sequential(nn.Linear(dataset.num_features, dim*2), nn.ReLU(), nn.Linear(dim*2, dim))\n",
    "        nn2 = nn.Sequential(nn.Linear(dim, dim*2), nn.ReLU(), nn.Linear(dim*2, dim))\n",
    "        self.conv1 = pyg_nn.GINConv(nn1)\n",
    "        self.conv2 = pyg_nn.GINConv(nn2)\n",
    "        self.fc1 = nn.Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, dim=d):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = pyg_nn.GATConv(dataset.num_features, dim, edge_dim=edge_attr_all.shape[1]) \n",
    "        self.conv2 = pyg_nn.GATConv(dim, dim, edge_dim=edge_attr_all.shape[1]) \n",
    "        self.fc1 = nn.Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, edge_attr_all\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class MPNN(torch.nn.Module):\n",
    "    def __init__(self, dim=d):\n",
    "        super(MPNN, self).__init__()\n",
    "        nn1 = nn.Sequential(nn.Linear(edge_attr_all.shape[1], 16), nn.ReLU(), nn.Linear(16, dataset.num_features*dim))\n",
    "        self.conv1 = pyg_nn.NNConv(dataset.num_features, dim, nn1)\n",
    "        nn2 = nn.Sequential(nn.Linear(edge_attr_all.shape[1], 16), nn.ReLU(), nn.Linear(16, dim*dim))\n",
    "        self.conv2 = pyg_nn.NNConv(dim, dim, nn2)\n",
    "        self.fc1 = nn.Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, edge_attr_all #data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class CGC(torch.nn.Module):\n",
    "    def __init__(self, dim=d):\n",
    "        super(CGC, self).__init__()\n",
    "        self.conv1 = pyg_nn.CGConv(dataset.num_features, edge_attr_all.size(-1))\n",
    "        self.conv2 = pyg_nn.CGConv(dataset.num_features, edge_attr_all.size(-1)) \n",
    "        self.fc1 = nn.Linear(dataset.num_features, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, edge_attr_all\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr)) # \n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    \n",
    "class GraphTransformer(torch.nn.Module):\n",
    "    def __init__(self, dim=d):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.conv1 = pyg_nn.TransformerConv(dataset.num_features, dim, edge_dim=edge_attr_all.shape[1]) \n",
    "        self.conv2 = pyg_nn.TransformerConv(dim, dim, edge_dim=edge_attr_all.shape[1]) \n",
    "        self.fc1 = nn.Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, edge_attr_all\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class GEN(torch.nn.Module):\n",
    "    def __init__(self, dim=d):\n",
    "        super(GEN, self).__init__()\n",
    "        self.node_encoder = nn.Linear(data.x.size(-1), dim)\n",
    "        self.edge_encoder = nn.Linear(edge_attr_all.size(-1), dim)\n",
    "        self.conv1 = pyg_nn.GENConv(dim, dim) \n",
    "        self.conv2 = pyg_nn.GENConv(dim, dim)\n",
    "        self.fc1 = nn.Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index, edge_attr = self.node_encoder(data.x), data.edge_index, self.edge_encoder(edge_attr_all)\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class TRAVELNet(torch.nn.Module):\n",
    "    def __init__(self, dim=d):\n",
    "        super(TRAVELNet, self).__init__()\n",
    "        convdim = 8\n",
    "        self.node_encoder = nn.Sequential(nn.Linear(data.x.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "        self.edge_encoder_dir = nn.Sequential(nn.Linear(data.component_dir.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "        self.edge_encoder_ang = nn.Sequential(nn.Linear(data.component_ang.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "        nn1 = nn.Sequential(nn.Linear(dim + dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, convdim))\n",
    "        self.conv1 = TRAVELConv(dim, convdim, nn1)\n",
    "        nn2 = nn.Sequential(nn.Linear(2*convdim + dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, dataset.num_classes))\n",
    "        self.conv2 = TRAVELConv(2*convdim, dataset.num_classes, nn2)\n",
    "        self.bn1 = nn.BatchNorm1d(convdim*2)\n",
    "        nn1_2 = nn.Sequential(nn.Linear(dim + dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, convdim))\n",
    "        self.conv1_2 = TRAVELConv(dim, convdim, nn1_2)\n",
    "        nn2_2 = nn.Sequential(nn.Linear(2*convdim + dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, dataset.num_classes))\n",
    "        self.conv2_2 = TRAVELConv(2*convdim, dataset.num_classes, nn2_2)\n",
    "        self.bn2 = nn.BatchNorm1d(dataset.num_classes*2)\n",
    "        self.fc = nn.Linear(dataset.num_classes*2, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = self.node_encoder(data.x), data.edge_index\n",
    "        edge_attr_dir, edge_attr_ang = self.edge_encoder_dir(data.component_dir), self.edge_encoder_ang(data.component_ang)\n",
    "        x1 = F.relu(self.conv1(x, edge_index, edge_attr_dir))\n",
    "        x2 = F.relu(self.conv1_2(x, edge_index, edge_attr_ang))\n",
    "        x = torch.cat((x1, x2), axis=1)\n",
    "        x = self.bn1(x)\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x1 = F.relu(self.conv2(x, edge_index, edge_attr_dir))\n",
    "        x2 = F.relu(self.conv2_2(x, edge_index, edge_attr_ang))\n",
    "        x = torch.cat((x1, x2), axis=1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "# class TRAVELAng(torch.nn.Module):\n",
    "#     def __init__(self, dim=d):\n",
    "#         super(TRAVELAng, self).__init__()\n",
    "#         self.node_encoder = nn.Sequential(nn.Linear(data.x.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "#         self.edge_encoder_ang = nn.Sequential(nn.Linear(data.component_ang.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "#         nn1 = nn.Sequential(nn.Linear(dim+dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "#         self.conv1 = TRAVELConv(dim, dim, nn1)\n",
    "#         self.bn1 = nn.BatchNorm1d(dim)\n",
    "#         nn2 = nn.Sequential(nn.Linear(dim+dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "#         self.conv2 = TRAVELConv(dim, dim, nn2)\n",
    "#         self.fc = nn.Linear(dim, dataset.num_classes)\n",
    "\n",
    "#     def forward(self):\n",
    "#         x, edge_index = self.node_encoder(data.x), data.edge_index\n",
    "#         edge_attr_encoded = self.edge_encoder_ang(data.component_ang)\n",
    "#         x = F.relu(self.conv1(x, edge_index, edge_attr_encoded))\n",
    "#         x = self.bn1(x)\n",
    "#         x = F.dropout(x, p=p, training=self.training)\n",
    "#         x = F.relu(self.conv2(x, edge_index, edge_attr_encoded))\n",
    "#         x = self.fc(x)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "# class TRAVELDir(torch.nn.Module):\n",
    "#     def __init__(self, dim=d):\n",
    "#         super(TRAVELDir, self).__init__()\n",
    "#         self.node_encoder = nn.Sequential(nn.Linear(data.x.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "#         self.edge_encoder_dir = nn.Sequential(nn.Linear(data.component_dir.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "#         nn1 = nn.Sequential(nn.Linear(dim+dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "#         self.conv1 = TRAVELConv(dim, dim, nn1)\n",
    "#         self.bn1 = nn.BatchNorm1d(dim)\n",
    "#         nn2 = nn.Sequential(nn.Linear(dim+dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "#         self.conv2 = TRAVELConv(dim, dim, nn2)\n",
    "#         self.fc = nn.Linear(dim, dataset.num_classes)\n",
    "\n",
    "#     def forward(self):\n",
    "#         x, edge_index = self.node_encoder(data.x), data.edge_index\n",
    "#         edge_attr_encoded = self.edge_encoder_dir(data.component_dir)\n",
    "#         x = F.relu(self.conv1(x, edge_index, edge_attr_encoded))\n",
    "#         x = self.bn1(x)\n",
    "#         x = F.dropout(x, p=p, training=self.training)\n",
    "#         x = F.relu(self.conv2(x, edge_index, edge_attr_encoded))\n",
    "#         x = self.fc(x)\n",
    "#         return F.log_softmax(x, dim=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79dce3-b721-4e1d-abed-52cbaeb99851",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef38c0-c413-4fcc-ba45-4f816625bf93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for e in [('Miami', 'Florida'),('Los Angeles', 'California'),('Orlando', 'Florida'),('Dallas', 'Texas'),('Houston', 'Texas'),('New York', 'New York'),\n",
    "          ('Charlotte', 'North Carolina'),('San Diego', 'California'),('Nashville', 'Tennessee'),('Sacramento', 'California')]:\n",
    "    print(e)\n",
    "    city_name, state_abbrev = e[0].lower().replace(\" \", \"_\"), us_state_to_abbrev[e[1]].lower()\n",
    "    city_format = e[0]+' ('+us_state_to_abbrev[e[1]]+')'\n",
    "    \n",
    "    if os.path.exists(path+'exp/'+city_name+'_'+state_abbrev+'/processed'):\n",
    "        shutil.rmtree(path+'exp/'+city_name+'_'+state_abbrev+'/processed')\n",
    "    dataset = TRAVELDataset(path+'exp/', city_name+'_'+state_abbrev)\n",
    "    data = dataset[0]\n",
    "    class_num = dataset.num_classes\n",
    "    \n",
    "    # print(f'Number of graphs: {len(dataset)}')\n",
    "    # print(f'Number of node features: {dataset.num_features}')\n",
    "    # print(f'Number of edge features: {dataset.num_edge_features}')\n",
    "    # print(f'Number of classes: {dataset.num_classes}')\n",
    "    # print(f'Number of nodes: {data.num_nodes}')\n",
    "    # print(f'Number of edges: {data.num_edges}')\n",
    "    # print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "    # print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
    "    # print(f'Contains self-loops: {data.has_self_loops()}')\n",
    "    # print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "    # 60%, 20% and 20% for training, validation and test\n",
    "    data.train_mask, data.val_mask, data.test_mask = train_test_split_stratify(dataset, train_ratio=0.6, val_ratio=0.2, class_num=class_num)\n",
    "    sc = MinMaxScaler()\n",
    "    data.x[data.train_mask] = torch.tensor(sc.fit_transform(data.x[data.train_mask]), dtype=torch.float)\n",
    "    data.x[data.val_mask] = torch.tensor(sc.transform(data.x[data.val_mask]), dtype=torch.float)\n",
    "    data.x[data.test_mask] = torch.tensor(sc.transform(data.x[data.test_mask]), dtype=torch.float)\n",
    "\n",
    "    edge_attr_all = MinMaxScaler().fit_transform(data.edge_attr.cpu()) \n",
    "    edge_attr_all = torch.tensor(edge_attr_all).float().to(device)\n",
    "\n",
    "    coords = data.coords.numpy()\n",
    "    gdf_pred = pd.DataFrame({'x': coords[:, 0], 'y': coords[:, 1], 'label': data.y.numpy()}) \n",
    "    zip_iterator = zip(gdf_pred.index, gdf_pred[['x', 'y']].values)\n",
    "    pos_dict = dict(zip_iterator)\n",
    "    draw_with_labels(gdf_pred, 'Ground Truth')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = data.x[data.train_mask].cpu().numpy(), data.x[data.test_mask].cpu().numpy(), data.y[data.train_mask].cpu().numpy(), data.y[data.test_mask].cpu().numpy()\n",
    "    start_time = time.time()\n",
    "    xgb_clf = XGBClassifier(objective='multi:softmax', eval_metric='mlogloss')\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "    y_pred = xgb_clf.predict(X_test)\n",
    "    test_acc, test_f1 = accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='weighted')\n",
    "    print('F measure {:.5f} | Test Accuracy {:.5f}'.format(test_f1, test_acc))\n",
    "    res = (round(test_f1*100, 2), round(test_acc*100, 2))\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('XGBoost',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "    y_pred_all = xgb_clf.predict(data.x.cpu().numpy())\n",
    "    df_pred = pd.DataFrame({'x': coords[:, 0], 'y': coords[:, 1], 'label': y_pred_all}) \n",
    "    draw_with_labels(df_pred, 'XGBoost')\n",
    "    \n",
    "    data = data.to(device) \n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = MLP().to(device) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'MLP')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('MLP',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = GCN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'GCN')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('GCN',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = ChebNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.02, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'ChebNet')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('ChebNet',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = ARMANet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.02, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'ARMANet')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('ARMANet',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = GraphSAGE().to(device) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'GraphSAGE')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('GraphSAGE',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = TAGCN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'TAGCN')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('TAGCN',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = GIN().to(device) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'GIN')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('GIN',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = GAT().to(device) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'GAT')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('GAT',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = MPNN().to(device)  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'MPNN')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('MPNN',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = CGC().to(device) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'CGC')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('CGC',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = GraphTransformer().to(device) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'Transformer')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('Transformer',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = GEN().to(device) \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'GEN')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('GEN',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "    component_dir = np.concatenate((data.edge_attr.cpu(), data.edge_attr_dir.cpu()), axis=1)\n",
    "    component_ang = np.concatenate((data.edge_attr.cpu(), data.edge_attr_ang.cpu()), axis=1)\n",
    "    component_dir = StandardScaler().fit_transform(component_dir)\n",
    "    component_ang = StandardScaler().fit_transform(component_ang)\n",
    "    data.component_dir = torch.tensor(component_dir).float().to(device)\n",
    "    data.component_ang = torch.tensor(component_ang).float().to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = TRAVELNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.006, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'TRAVEL')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('TRAVEL',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     model = TRAVELAng().to(device)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.006, weight_decay=5e-4)\n",
    "#     res = train_loop(model, data, optimizer, num_epochs, 'TRAVEL-ang')\n",
    "#     t = round(time.time() - start_time, 2)\n",
    "#     all_res.append((city_format,) + ('TRAVEL-ang',) + res + (t,))\n",
    "#     print(\"Execution time: %.4f seconds\" % t)\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     model = TRAVELDir().to(device)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.006, weight_decay=5e-4)\n",
    "#     res = train_loop(model, data, optimizer, num_epochs, 'TRAVEL-dir')\n",
    "#     t = round(time.time() - start_time, 2)\n",
    "#     all_res.append((city_format,) + ('TRAVEL-dir',) + res + (t,))\n",
    "#     print(\"Execution time: %.4f seconds\" % t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d9457398-77ed-4a9f-b5fd-7cf65bb7e31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# datasets: 693\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Method</th>\n",
       "      <th>Weighted F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miami (FL)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>81.26</td>\n",
       "      <td>86.82</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Miami (FL)</td>\n",
       "      <td>MLP</td>\n",
       "      <td>81.26</td>\n",
       "      <td>86.82</td>\n",
       "      <td>9.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miami (FL)</td>\n",
       "      <td>GCN</td>\n",
       "      <td>81.13</td>\n",
       "      <td>86.59</td>\n",
       "      <td>10.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miami (FL)</td>\n",
       "      <td>ChebNet</td>\n",
       "      <td>81.40</td>\n",
       "      <td>86.76</td>\n",
       "      <td>10.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Miami (FL)</td>\n",
       "      <td>ARMANet</td>\n",
       "      <td>81.13</td>\n",
       "      <td>86.59</td>\n",
       "      <td>10.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11776</th>\n",
       "      <td>Hatfield Township (PA)</td>\n",
       "      <td>GEN</td>\n",
       "      <td>73.76</td>\n",
       "      <td>74.34</td>\n",
       "      <td>9.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11777</th>\n",
       "      <td>Hatfield Township (PA)</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>74.47</td>\n",
       "      <td>75.22</td>\n",
       "      <td>11.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11778</th>\n",
       "      <td>Hatfield Township (PA)</td>\n",
       "      <td>TRAVEL-ang</td>\n",
       "      <td>72.34</td>\n",
       "      <td>73.45</td>\n",
       "      <td>10.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11779</th>\n",
       "      <td>Hatfield Township (PA)</td>\n",
       "      <td>TRAVEL-dir</td>\n",
       "      <td>79.75</td>\n",
       "      <td>80.53</td>\n",
       "      <td>10.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11780</th>\n",
       "      <td>Hatfield Township (PA)</td>\n",
       "      <td>TRAVEL-rm</td>\n",
       "      <td>76.77</td>\n",
       "      <td>76.99</td>\n",
       "      <td>10.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11781 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         City      Method  Weighted F1  Accuracy   Time\n",
       "0                  Miami (FL)     XGBoost        81.26     86.82   0.90\n",
       "1                  Miami (FL)         MLP        81.26     86.82   9.64\n",
       "2                  Miami (FL)         GCN        81.13     86.59  10.35\n",
       "3                  Miami (FL)     ChebNet        81.40     86.76  10.38\n",
       "4                  Miami (FL)     ARMANet        81.13     86.59  10.48\n",
       "...                       ...         ...          ...       ...    ...\n",
       "11776  Hatfield Township (PA)         GEN        73.76     74.34   9.03\n",
       "11777  Hatfield Township (PA)      TRAVEL        74.47     75.22  11.93\n",
       "11778  Hatfield Township (PA)  TRAVEL-ang        72.34     73.45  10.10\n",
       "11779  Hatfield Township (PA)  TRAVEL-dir        79.75     80.53  10.14\n",
       "11780  Hatfield Township (PA)   TRAVEL-rm        76.77     76.99  10.03\n",
       "\n",
       "[11781 rows x 5 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_res, columns=['City', 'Method', 'Weighted F1', 'Accuracy', 'Time'])\n",
    "print('# datasets:', df.shape[0] // len(df.Method.unique()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "v12_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
